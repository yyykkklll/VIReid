"""
models/memory_bank.py - å¤šç²’åº¦è®°å¿†åº“æ¨¡å— (å®Œæ•´ä¿®å¤ç‰ˆ)

ä¿®å¤æ—¥å¿—:
1. [P0] æ·»åŠ update_memory()ä¸­çš„labelèŒƒå›´æ£€æŸ¥
2. ä¼˜åŒ–initialize_memory()çš„é”™è¯¯æç¤ºä¿¡æ¯
3. æ·»åŠ è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š

åŠŸèƒ½:
ç»´æŠ¤ä¸€ä¸ªä¸‰ç»´è®°å¿†åº“ M âˆˆ R^(K Ã— N Ã— D)
- K: éƒ¨ä»¶æ•°é‡
- N: èº«ä»½æ•°é‡  
- D: ç‰¹å¾ç»´åº¦
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class MultiPartMemoryBank(nn.Module):
    """
    å¤šç²’åº¦å¼ é‡è®°å¿†åº“
    
    å­˜å‚¨ç»“æ„:
    - memory: Tensor[K, N, D]ï¼Œæ¯ä¸ªéƒ¨ä»¶-èº«ä»½ç»„åˆçš„ç‰¹å¾åŸå‹
    - initialized: Tensor[N]ï¼Œæ ‡è®°æ¯ä¸ªèº«ä»½æ˜¯å¦å·²åˆå§‹åŒ–
    
    æ›´æ–°ç­–ç•¥:
    åŠ¨é‡æ›´æ–°: M[k, y] â† m * M[k, y] + (1-m) * f_new
    """
    def __init__(self, num_parts, num_identities, feature_dim, momentum=0.9):
        """
        Args:
            num_parts: éƒ¨ä»¶æ•°é‡ K
            num_identities: èº«ä»½æ•°é‡ N
            feature_dim: ç‰¹å¾ç»´åº¦ D
            momentum: åŠ¨é‡æ›´æ–°ç³»æ•° mï¼ŒèŒƒå›´[0, 1]
        """
        super(MultiPartMemoryBank, self).__init__()
        self.num_parts = num_parts
        self.num_identities = num_identities
        self.feature_dim = feature_dim
        self.momentum = momentum
        
        # åˆå§‹åŒ–è®°å¿†åº“ [K, N, D]
        # ä½¿ç”¨register_bufferä½¿å…¶ä½œä¸ºæ¨¡å‹çŠ¶æ€ä¿å­˜ï¼Œä½†ä¸å‚ä¸æ¢¯åº¦æ›´æ–°
        self.register_buffer(
            'memory',
            F.normalize(torch.randn(num_parts, num_identities, feature_dim), dim=2)
        )
        
        # è®°å½•æ¯ä¸ªIDæ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆé¦–æ¬¡å‡ºç°ï¼‰
        self.register_buffer(
            'initialized',
            torch.zeros(num_identities, dtype=torch.bool)
        )
    
    @torch.no_grad()
    def initialize_memory(self, part_features, labels):
        """
        ä½¿ç”¨åˆå§‹æ•°æ®æ‰¹é‡åˆå§‹åŒ–è®°å¿†åº“
        é€šå¸¸åœ¨è®­ç»ƒå¼€å§‹å‰ç”¨æ•´ä¸ªè®­ç»ƒé›†çš„RGBæ¨¡æ€æ•°æ®åˆå§‹åŒ–
        
        Args:
            part_features: List[Tensor[B, D]]ï¼ŒKä¸ªéƒ¨ä»¶ç‰¹å¾
            labels: Tensor[B]ï¼Œæ ·æœ¬æ ‡ç­¾
        """
        K = len(part_features)
        assert K == self.num_parts, f"Expected {self.num_parts} parts, got {K}"
        
        # [ä¿®å¤] æ£€æŸ¥labelèŒƒå›´ï¼Œæä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯
        unique_labels = labels.unique()
        max_label = unique_labels.max().item()
        min_label = unique_labels.min().item()
        
        print(f"  ğŸ“Š Label range in batch: [{min_label}, {max_label}]")
        print(f"  ğŸ“Š Memory bank size: [K={K}, N={self.num_identities}, D={self.feature_dim}]")
        
        # å¦‚æœlabelè¶…å‡ºèŒƒå›´ï¼ŒæŠ›å‡ºè¯¦ç»†é”™è¯¯
        if max_label >= self.num_identities:
            raise ValueError(
                f"\n{'='*60}\n"
                f"âŒ ERROR: Label out of range!\n"
                f"   Max label in data: {max_label}\n"
                f"   Memory bank size: {self.num_identities}\n"
                f"   This error occurs because the dataloader did not properly\n"
                f"   map the original IDs to continuous indices [0, N-1].\n"
                f"   Please check 'dataloader_adapter.py' for label mapping.\n"
                f"{'='*60}"
            )
        
        # å¯¹æ¯ä¸ªéƒ¨ä»¶åˆ†åˆ«åˆå§‹åŒ–
        for k in range(K):
            features = part_features[k]  # [B, D]
            features = F.normalize(features, dim=1)  # L2å½’ä¸€åŒ–
            
            # æŒ‰æ ‡ç­¾èšåˆç‰¹å¾ï¼ˆå–åŒIDæ ·æœ¬çš„å‡å€¼ï¼‰
            for label in unique_labels:
                label_idx = label.item()
                mask = (labels == label)  # å½“å‰IDçš„æ ·æœ¬mask
                
                if mask.sum() > 0:
                    # è®¡ç®—è¯¥IDæ‰€æœ‰æ ·æœ¬çš„å¹³å‡ç‰¹å¾
                    mean_feature = features[mask].mean(dim=0)  # [D]
                    
                    # å½’ä¸€åŒ–åå­˜å…¥è®°å¿†åº“ï¼ˆä½¿ç”¨cloneé¿å…inplaceè­¦å‘Šï¼‰
                    normalized_feature = F.normalize(mean_feature.unsqueeze(0), dim=1).squeeze(0)
                    self.memory[k, label_idx] = normalized_feature.clone()
                    self.initialized[label_idx] = True
        
        num_initialized = self.initialized.sum().item()
        print(f"  âœ… Initialized: {num_initialized}/{self.num_identities} identities")
    
    @torch.no_grad()
    def update_memory(self, part_features, labels):
        """
        åŠ¨é‡æ›´æ–°è®°å¿†åº“
        åœ¨æ¯ä¸ªè®­ç»ƒbatchåè°ƒç”¨ï¼Œæ›´æ–°å½“å‰batchæ¶‰åŠçš„IDçš„è®°å¿†
        
        å…¬å¼: M[k, y] â† m * M[k, y] + (1-m) * f_id^(k)
        
        Args:
            part_features: List[Tensor[B, D]]ï¼ŒKä¸ªéƒ¨ä»¶ç‰¹å¾
            labels: Tensor[B]ï¼Œæ ·æœ¬æ ‡ç­¾
        """
        K = len(part_features)
        m = self.momentum
        
        # [ä¿®å¤] æ£€æŸ¥labelèŒƒå›´ï¼Œé˜²æ­¢IndexError
        max_label = labels.max().item()
        if max_label >= self.num_identities:
            raise ValueError(
                f"âŒ Label {max_label} exceeds memory bank size {self.num_identities}. "
                f"Check your dataloader label mapping!"
            )
        
        # åˆ›å»ºæ–°çš„è®°å¿†åº“å‰¯æœ¬ï¼Œé¿å…inplaceæ“ä½œå¯¼è‡´çš„æ¢¯åº¦é—®é¢˜
        new_memory = self.memory.clone()
        
        for k in range(K):
            features = part_features[k]  # [B, D]
            features = F.normalize(features, dim=1)  # L2å½’ä¸€åŒ–
            
            # æ›´æ–°æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„è®°å¿†
            for i, label in enumerate(labels):
                label_idx = label.item()
                old_memory = self.memory[k, label_idx]  # [D] æ—§è®°å¿†
                new_feature = features[i]  # [D] æ–°ç‰¹å¾
                
                # åŠ¨é‡æ›´æ–°
                if self.initialized[label_idx]:
                    # å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨åŠ¨é‡èåˆ
                    updated = m * old_memory + (1 - m) * new_feature
                else:
                    # é¦–æ¬¡å‡ºç°ï¼Œç›´æ¥èµ‹å€¼
                    updated = new_feature
                    self.initialized[label_idx] = True
                
                # å½’ä¸€åŒ–å¹¶æ›´æ–°åˆ°æ–°è®°å¿†åº“
                new_memory[k, label_idx] = F.normalize(updated.unsqueeze(0), dim=1).squeeze(0)
        
        # ä¸€æ¬¡æ€§æ›´æ–°æ•´ä¸ªè®°å¿†åº“ï¼ˆé¿å…å¤šæ¬¡å†™å…¥ï¼‰
        self.memory.copy_(new_memory)
    
    def get_memory(self, part_idx=None):
        """
        è·å–è®°å¿†åº“
        
        Args:
            part_idx: éƒ¨ä»¶ç´¢å¼• (0 ~ K-1)ï¼ŒNoneè¡¨ç¤ºè¿”å›å…¨éƒ¨
        Returns:
            memory: Tensor[N, D] or Tensor[K, N, D]
        """
        if part_idx is not None:
            return self.memory[part_idx]  # [N, D]
        return self.memory  # [K, N, D]
    
    def get_part_memory(self, part_idx):
        """
        è·å–æŒ‡å®šéƒ¨ä»¶çš„è®°å¿†åº“
        
        Args:
            part_idx: éƒ¨ä»¶ç´¢å¼• (0 ~ K-1)
        Returns:
            memory: Tensor[N, D]
        """
        return self.memory[part_idx]
    
    def forward(self, part_features):
        """
        è®¡ç®—ç‰¹å¾ä¸è®°å¿†åº“çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            part_features: List[Tensor[B, D]]ï¼ŒKä¸ªéƒ¨ä»¶ç‰¹å¾
        Returns:
            similarities: List[Tensor[B, N]]ï¼ŒKä¸ªç›¸ä¼¼åº¦çŸ©é˜µ
        """
        similarities = []
        for k in range(self.num_parts):
            features = F.normalize(part_features[k], dim=1)  # [B, D]
            memory = self.memory[k]  # [N, D]
            
            # ä½™å¼¦ç›¸ä¼¼åº¦: sim(f, m) = f Â· m^T
            sim = torch.mm(features, memory.t())  # [B, N]
            similarities.append(sim)
        
        return similarities


class AdaptiveMemoryBank(MultiPartMemoryBank):
    """
    è‡ªé€‚åº”è®°å¿†åº“ (é«˜çº§ç‰ˆæœ¬)
    
    åœ¨åŸºç¡€è®°å¿†åº“ä¸Šå¢åŠ äº†ç½®ä¿¡åº¦è¿‡æ»¤æœºåˆ¶:
    - ä½ç½®ä¿¡åº¦æ ·æœ¬ä¸å‚ä¸è®°å¿†åº“æ›´æ–°
    - ç»´æŠ¤æ¯ä¸ªIDçš„å…¨å±€ç½®ä¿¡åº¦åˆ†æ•°
    """
    def __init__(self, num_parts, num_identities, feature_dim,
                 momentum=0.9, confidence_threshold=0.5):
        """
        Args:
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„æ ·æœ¬è¢«è¿‡æ»¤
        """
        super().__init__(num_parts, num_identities, feature_dim, momentum)
        self.confidence_threshold = confidence_threshold
        
        # è®°å½•æ¯ä¸ªIDçš„ç½®ä¿¡åº¦
        self.register_buffer(
            'confidence',
            torch.zeros(num_identities)
        )
    
    @torch.no_grad()
    def update_memory(self, part_features, labels, confidences=None):
        """
        å¸¦ç½®ä¿¡åº¦çš„åŠ¨é‡æ›´æ–°
        
        Args:
            part_features: List[Tensor[B, D]]ï¼ŒKä¸ªéƒ¨ä»¶ç‰¹å¾
            labels: Tensor[B]ï¼Œæ ·æœ¬æ ‡ç­¾
            confidences: Tensor[B] (å¯é€‰)ï¼Œç½®ä¿¡åº¦åˆ†æ•°
        """
        K = len(part_features)
        m = self.momentum
        
        # åˆ›å»ºæ–°çš„è®°å¿†åº“å’Œç½®ä¿¡åº¦å‰¯æœ¬
        new_memory = self.memory.clone()
        new_confidence = self.confidence.clone()
        
        for k in range(K):
            features = part_features[k]  # [B, D]
            features = F.normalize(features, dim=1)
            
            for i, label in enumerate(labels):
                label_idx = label.item()
                
                # æ£€æŸ¥ç½®ä¿¡åº¦ï¼ˆå¦‚æœæä¾›ï¼‰
                if confidences is not None:
                    conf = confidences[i].item()
                    if conf < self.confidence_threshold:
                        continue  # è·³è¿‡ä½ç½®ä¿¡åº¦æ ·æœ¬
                    
                    # æ›´æ–°å…¨å±€ç½®ä¿¡åº¦ï¼ˆåŠ¨é‡å¹³å‡ï¼‰
                    new_confidence[label_idx] = m * self.confidence[label_idx] + (1 - m) * conf
                
                # æ›´æ–°è®°å¿†
                old_memory = self.memory[k, label_idx]
                new_feature = features[i]
                
                if self.initialized[label_idx]:
                    updated = m * old_memory + (1 - m) * new_feature
                else:
                    updated = new_feature
                    self.initialized[label_idx] = True
                
                new_memory[k, label_idx] = F.normalize(updated.unsqueeze(0), dim=1).squeeze(0)
        
        # ä¸€æ¬¡æ€§æ›´æ–°
        self.memory.copy_(new_memory)
        self.confidence.copy_(new_confidence)
