"""
models/pfmgcd_model.py - PF-MGCD Ultimate (Modular Architecture)

åŠŸèƒ½æ¨¡å—:
1. Backbone: IBN-Net + GeM Pooling (åŸºç¡€åº•åº§)
2. ISG-DM: è§£è€¦æ¨¡å—
3. [ç­–ç•¥ä¸‰] Modality Adversarial: æ¢¯åº¦åè½¬å±‚ (GRL) + æ¨¡æ€åˆ¤åˆ«å™¨
4. [ç­–ç•¥å››] Graph Reasoning: åŸºäºè®°å¿†åº“çš„ GCN ç‰¹å¾å¢å¼º
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Function

from .pcb_backbone import PCBBackbone, GeMPooling
from .isg_dm import MultiPartISG_DM
from .memory_bank import MultiPartMemoryBank
from .graph_propagation import AdaptiveGraphPropagation

# ==================== åŸºç¡€ç»„ä»¶ ====================

def weights_init_kaiming(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)
    elif classname.find('BatchNorm1d') != -1:
        nn.init.normal_(m.weight, 1.0, 0.01)
        nn.init.constant_(m.bias, 0.0)

class GradientReversalFunction(Function):
    """æ¢¯åº¦åè½¬å±‚ (GRL) çš„æ ¸å¿ƒå®ç°"""
    @staticmethod
    def forward(ctx, x, lambda_):
        ctx.lambda_ = lambda_
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        # åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦å–åå¹¶ä¹˜ä»¥ lambda
        return grad_output.neg() * ctx.lambda_, None

class GradientReversal(nn.Module):
    def __init__(self, lambda_=1.0):
        super(GradientReversal, self).__init__()
        self.lambda_ = lambda_

    def forward(self, x):
        return GradientReversalFunction.apply(x, self.lambda_)

# ==================== ç­–ç•¥ä¸‰ï¼šæ¨¡æ€åˆ¤åˆ«å™¨ ====================

class ModalityDiscriminator(nn.Module):
    """
    æ¨¡æ€åˆ¤åˆ«å™¨: è¯•å›¾åˆ†è¾¨ç‰¹å¾æ˜¯æ¥è‡ª RGB è¿˜æ˜¯ IR
    å¯¹æŠ—ç›®æ ‡: æå–å™¨ç”Ÿæˆçš„ç‰¹å¾è®©åˆ¤åˆ«å™¨åˆ†ä¸æ¸… (Prob -> 0.5)
    """
    def __init__(self, input_dim):
        super(ModalityDiscriminator, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.BatchNorm1d(input_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(input_dim // 2, 2)  # 2ç±»: RGB vs IR
        )
        self.classifier.apply(weights_init_kaiming)

    def forward(self, x):
        return self.classifier(x)

# ==================== ç­–ç•¥å››ï¼šå›¾æ¨ç†æ¨¡å— ====================

class GraphReasoning(nn.Module):
    """
    å›¾æ¨ç†æ¨¡å— (GCN on Memory)
    åˆ©ç”¨è®°å¿†åº“ä¸­çš„ Top-K é‚»å±…ç‰¹å¾æ¥å¢å¼ºå½“å‰æŸ¥è¯¢ç‰¹å¾
    """
    def __init__(self, feature_dim, top_k=5):
        super(GraphReasoning, self).__init__()
        self.top_k = top_k
        self.feature_dim = feature_dim
        
        # GCN æƒé‡
        self.gcn_weight = nn.Linear(feature_dim, feature_dim)
        self.relu = nn.ReLU(inplace=True)
        # èåˆé—¨æ§
        self.fusion = nn.Linear(feature_dim * 2, feature_dim)
        
        self._init_weights()

    def _init_weights(self):
        self.gcn_weight.apply(weights_init_kaiming)
        self.fusion.apply(weights_init_kaiming)

    def forward(self, part_feature, memory):
        """
        Args:
            part_feature: [B, D]
            memory: [N, D] è®°å¿†åº“ç‰¹å¾
        Returns:
            enhanced_feature: [B, D]
        """
        B, D = part_feature.size()
        
        # 1. æ£€ç´¢ Top-K é‚»å±…
        # å½’ä¸€åŒ–
        feat_norm = F.normalize(part_feature, p=2, dim=1)
        mem_norm = F.normalize(memory, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦ [B, N]
        sim = torch.mm(feat_norm, mem_norm.t())
        
        # è·å– Top-K [B, K]
        topk_val, topk_idx = torch.topk(sim, k=self.top_k, dim=1)
        
        # 2. æ„å»ºå±€éƒ¨å›¾ç‰¹å¾
        # æ”¶é›†é‚»å±…ç‰¹å¾ [B, K, D]
        neighbor_feats = F.embedding(topk_idx, memory)
        
        # 3. ç®€åŒ–çš„ GCN èšåˆ
        # A_ij = softmax(sim_ij)
        affinity = F.softmax(topk_val * 10, dim=1).unsqueeze(2) # [B, K, 1]
        
        # Aggregation: sum(A * W * X_neighbor)
        weighted_neighbors = (neighbor_feats * affinity).sum(dim=1) # [B, D]
        gcn_out = self.relu(self.gcn_weight(weighted_neighbors))
        
        # 4. æ®‹å·®èåˆ
        # concat [original, gcn] -> fuse
        fused = torch.cat([part_feature, gcn_out], dim=1)
        out = self.fusion(fused)
        
        return out + part_feature # Residual connection

# ==================== ä¸»æ¨¡å‹ ====================

class PF_MGCD(nn.Module):
    def __init__(self, num_parts=6, num_identities=395, feature_dim=512,
                 memory_momentum=0.9, temperature=3.0, top_k=5, 
                 pretrained=True, backbone='resnet50', use_ibn=True,
                 # æ–°å¢å¼€å…³å‚æ•°
                 use_adversarial=False, use_graph_reasoning=False):
        super(PF_MGCD, self).__init__()
        self.num_parts = num_parts
        self.feature_dim = feature_dim
        self.use_adversarial = use_adversarial
        self.use_graph_reasoning = use_graph_reasoning
        
        # 1. Backbone
        self.backbone = PCBBackbone(
            num_parts=num_parts, 
            pretrained=pretrained, 
            backbone=backbone,
            use_ibn=use_ibn
        )
        
        # 2. ISG-DM
        self.isg_dm = MultiPartISG_DM(
            num_parts=num_parts,
            input_dim=2048,
            id_dim=feature_dim,
            mod_dim=feature_dim
        )
        
        # 3. GeM Pooling
        self.gem_poolings = nn.ModuleList([
            GeMPooling(p=3.0) for _ in range(num_parts)
        ])
        
        # --- ç­–ç•¥ä¸‰ï¼šæ¨¡æ€å¯¹æŠ— ---
        if self.use_adversarial:
            self.grl = GradientReversal(lambda_=0.1) # æ¢¯åº¦åè½¬
            self.mod_discriminators = nn.ModuleList([
                ModalityDiscriminator(feature_dim) for _ in range(num_parts)
            ])
            print("âœ… Modality Adversarial Learning Enabled.")
            
        # --- ç­–ç•¥å››ï¼šå›¾æ¨ç† ---
        if self.use_graph_reasoning:
            self.graph_reasoning_modules = nn.ModuleList([
                GraphReasoning(feature_dim, top_k=top_k) for _ in range(num_parts)
            ])
            print("âœ… Graph Reasoning (GCN) Enabled.")

        # 4. BNNeck & Classifiers
        self.bottlenecks = nn.ModuleList([
            nn.BatchNorm1d(feature_dim) for _ in range(num_parts)
        ])
        self.id_classifiers = nn.ModuleList([
            nn.Linear(feature_dim, num_identities, bias=False) for _ in range(num_parts)
        ])
        
        self.bottlenecks.apply(weights_init_kaiming)
        self.id_classifiers.apply(weights_init_kaiming)
        self.dropout = nn.Dropout(p=0.5)
        
        # 5. Memory Bank
        # å¦‚æœä½¿ç”¨å›¾æ¨ç†ï¼Œå¿…é¡»å¯ç”¨è®°å¿†åº“
        self.memory_bank = MultiPartMemoryBank(
            num_parts=num_parts,
            num_identities=num_identities,
            feature_dim=feature_dim,
            momentum=memory_momentum
        )
        
        # 6. Graph Propagation (Loss)
        # å³ä½¿ä½¿ç”¨äº† GCNï¼Œè¿™ä¸ªæ¨¡å—ä¹Ÿå¯ä»¥ä¿ç•™ç”¨äºè®¡ç®— Soft Labels æŸå¤±
        self.graph_propagation = AdaptiveGraphPropagation(
            temperature=temperature,
            top_k=top_k,
            use_entropy_weight=True,
            scale=30.0
        )

    def forward(self, x, labels=None, **kwargs):
        # 1. Backbone
        output = self.backbone(x)
        if isinstance(output, tuple): part_features = output[0]
        else: part_features = output
        
        # 2. ISG-DM
        id_features_raw, mod_features = self.isg_dm(part_features)
        
        id_features = [] # æœ€ç»ˆç”¨äºåˆ†ç±»çš„ç‰¹å¾
        adv_logits = []  # å¯¹æŠ—åˆ¤åˆ«å™¨è¾“å‡º
        
        # 3. é€éƒ¨ä»¶å¤„ç†
        for k in range(self.num_parts):
            feat = id_features_raw[k] # [B, D]
            
            # [ç­–ç•¥å››] å›¾æ¨ç†å¢å¼º
            if self.use_graph_reasoning and self.memory_bank.initialized.sum() > 0:
                # ä½¿ç”¨è®°å¿†åº“ä¸­çš„ç‰¹å¾è¿›è¡Œ GCN æ›´æ–°
                # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ detach çš„ memory é˜²æ­¢æ¢¯åº¦å›ä¼ åˆ° memory (å®ƒæ˜¯ buffer)
                mem_k = self.memory_bank.get_part_memory(k).detach()
                feat = self.graph_reasoning_modules[k](feat, mem_k)
            
            # [ç­–ç•¥ä¸‰] æ¨¡æ€å¯¹æŠ—
            if self.training and self.use_adversarial:
                # æ¢¯åº¦åè½¬ -> åˆ¤åˆ«å™¨
                feat_rev = self.grl(feat)
                mod_logit = self.mod_discriminators[k](feat_rev)
                adv_logits.append(mod_logit)
            
            id_features.append(feat)

        # 4. BNNeck + Classifier
        id_logits = []
        graph_logits = []
        
        for k in range(self.num_parts):
            feat_bn = self.bottlenecks[k](id_features[k])
            
            if self.training:
                graph_logits.append(self.id_classifiers[k](feat_bn))
                id_logits.append(self.id_classifiers[k](self.dropout(feat_bn)))
            else:
                logit = self.id_classifiers[k](feat_bn)
                id_logits.append(logit)
                graph_logits.append(logit)
        
        # 5. Graph Propagation (Loss Calculation)
        # å¦‚æœå¯ç”¨äº†å›¾æ¨ç†ï¼Œè¿™é‡Œçš„ id_features å·²ç»æ˜¯å¢å¼ºè¿‡çš„
        if self.training and self.memory_bank.initialized.sum() > 0:
            soft_labels, _, entropy_weights = self.graph_propagation(id_features, self.memory_bank)
        else:
            soft_labels, entropy_weights = None, None
        
        outputs = {
            'id_features': id_features,
            'id_logits': id_logits, 
            'graph_logits': graph_logits,
            'soft_labels': soft_labels,
            'entropy_weights': entropy_weights,
            'adv_logits': adv_logits if self.use_adversarial else None # è¿”å›å¯¹æŠ—Logits
        }
        return outputs
    
    def extract_features(self, x, pool_parts=True):
        with torch.no_grad():
            output = self.backbone(x)
            if isinstance(output, tuple): part_features = output[0]
            else: part_features = output
            id_features_raw, _ = self.isg_dm(part_features)
            
            bn_features = []
            for k in range(self.num_parts):
                feat = id_features_raw[k]
                # æµ‹è¯•æ—¶é€šå¸¸ä¸å¼€å¯ GCN æ¨ç†ï¼Œä»¥ä¿æŒé«˜æ•ˆå’Œç¨³å®š
                # å¦‚æœæƒ³æè‡´æ€§èƒ½ï¼Œå¯ä»¥å¼€å¯ï¼Œä½†éœ€è¦åŠ è½½è®­ç»ƒå¥½çš„ Memory
                # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œåªç”¨ Backbone ç‰¹å¾
                feat_bn = self.bottlenecks[k](feat)
                bn_features.append(feat_bn)
            
            norm_features = [F.normalize(f, p=2, dim=1) for f in bn_features]
            
            if pool_parts:
                return torch.cat(norm_features, dim=1)
            else:
                return torch.stack(norm_features, dim=1).mean(dim=1)

    def initialize_memory(self, dataloader, device, teacher_model=None):
        self.eval()
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è®°å¿†åº“...")
        all_features = [[] for _ in range(self.num_parts)]
        all_labels = []
        with torch.no_grad():
            for batch in dataloader:
                if len(batch) == 3: imgs, pids, _ = batch
                else: imgs, info = batch; pids = info[:, 1]
                imgs = imgs.to(device)
                
                output = self.backbone(imgs)
                if isinstance(output, tuple): part_features = output[0]
                else: part_features = output
                id_features_raw, _ = self.isg_dm(part_features)
                
                for k in range(self.num_parts):
                    all_features[k].append(id_features_raw[k].cpu())
                all_labels.append(pids)
        
        for k in range(self.num_parts):
            all_features[k] = torch.cat(all_features[k], dim=0).to(device)
        all_labels = torch.cat(all_labels, dim=0).long().to(device)
        self.memory_bank.initialize_memory(all_features, all_labels)
        self.train()
        print("âœ… è®°å¿†åº“åˆå§‹åŒ–å®Œæˆ!")