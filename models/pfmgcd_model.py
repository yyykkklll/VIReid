"""
models/pfmgcd_model.py - PF-MGCDä¸»æ¨¡å‹ (å®Œæ•´ä¿®å¤ç‰ˆ)

ä¿®å¤æ—¥å¿—:
1. [P2] ä¿®å¤extract_featuresä¸­pool_partså‚æ•°æœªç”Ÿæ•ˆçš„é—®é¢˜
2. ä¼˜åŒ–è®°å¿†åº“æ¢å¤æ£€æŸ¥é€»è¾‘
3. æ·»åŠ è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Š

æ¨¡å‹æ¶æ„:
1. PCB Backbone (ResNet50) - äººä½“éƒ¨ä»¶åˆ‡åˆ†
2. ISG-DM - èº«ä»½/æ¨¡æ€è§£è€¦
3. Transformer - éƒ¨ä»¶ä¸Šä¸‹æ–‡äº¤äº’
4. BNNeck - å½’ä¸€åŒ–ç“¶é¢ˆå±‚
5. Classifier - èº«ä»½åˆ†ç±»å™¨
6. Memory Bank - å¤šç²’åº¦è®°å¿†åº“
7. Graph Propagation - å›¾ä¼ æ’­æ¨¡å—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

from .pcb_backbone import PCBBackbone
from .isg_dm import MultiPartISG_DM
from .memory_bank import MultiPartMemoryBank
from .graph_propagation import AdaptiveGraphPropagation


def weights_init_kaiming(m):
    """
    Kaimingåˆå§‹åŒ–ï¼ˆHeåˆå§‹åŒ–ï¼‰
    é€‚ç”¨äºReLUæ¿€æ´»å‡½æ•°çš„ç½‘ç»œ
    """
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)
    elif classname.find('BatchNorm1d') != -1:
        nn.init.normal_(m.weight, 1.0, 0.01)
        nn.init.constant_(m.bias, 0.0)


class PartContextTransformer(nn.Module):
    """
    éƒ¨ä»¶ä¸Šä¸‹æ–‡äº¤äº’æ¨¡å—
    
    ä½¿ç”¨Transformer Encoderæ•è·éƒ¨ä»¶é—´çš„ç©ºé—´å…³ç³»
    ä¾‹å¦‚ï¼šå¤´éƒ¨ç‰¹å¾å¯ä»¥å¸®åŠ©ä¸Šèº«ç‰¹å¾çš„åˆ¤åˆ«
    """
    def __init__(self, feature_dim, nhead=8, num_layers=1, dropout=0.1):
        """
        Args:
            feature_dim: ç‰¹å¾ç»´åº¦D
            nhead: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            num_layers: Transformerå±‚æ•°
            dropout: Dropoutæ¦‚ç‡
        """
        super(PartContextTransformer, self).__init__()
        
        # Transformerç¼–ç å™¨å±‚
        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=feature_dim,
            nhead=nhead,
            dim_feedforward=feature_dim * 4,  # FFNéšå±‚ç»´åº¦
            dropout=dropout,
            batch_first=True  # è¾“å…¥æ ¼å¼ä¸º[B, K, D]
        )
        
        # å †å å¤šå±‚Transformer
        self.transformer = nn.TransformerEncoder(
            self.encoder_layer, 
            num_layers=num_layers
        )
    
    def forward(self, part_features_list):
        """
        Args:
            part_features_list: List[Tensor[B, D]]ï¼ŒKä¸ªéƒ¨ä»¶ç‰¹å¾
        Returns:
            enhanced_features: List[Tensor[B, D]]ï¼Œäº¤äº’å¢å¼ºåçš„ç‰¹å¾
        """
        # å°†Listè½¬æ¢ä¸ºTensor: [B, K, D]
        x = torch.stack(part_features_list, dim=1)
        
        # Transformerç¼–ç 
        x = self.transformer(x)  # [B, K, D]
        
        # è½¬å›Listæ ¼å¼
        return [x[:, i, :] for i in range(x.size(1))]


class PF_MGCD(nn.Module):
    """
    PF-MGCDä¸»æ¨¡å‹
    
    Part-Based Fine-Grained Multi-Granularity Cross-Modal Distillation
    for Visible-Infrared Person Re-Identification
    """
    def __init__(self, num_parts=6, num_identities=395, feature_dim=512,
                 memory_momentum=0.9, temperature=3.0, top_k=5, 
                 pretrained=True, backbone='resnet50'):
        """
        Args:
            num_parts: éƒ¨ä»¶æ•°é‡Kï¼ˆé»˜è®¤6ï¼šå¤´-ä¸Šèº«-ä¸‹èº«-è…¿éƒ¨ç­‰ï¼‰
            num_identities: èº«ä»½æ•°é‡Nï¼ˆè®­ç»ƒé›†IDæ•°ï¼‰
            feature_dim: è§£è€¦åçš„ç‰¹å¾ç»´åº¦D
            memory_momentum: è®°å¿†åº“åŠ¨é‡ç³»æ•°
            temperature: å›¾ä¼ æ’­æ¸©åº¦
            top_k: Top-Ké‚»å±…æ•°
            pretrained: æ˜¯å¦ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡
            backbone: éª¨å¹²ç½‘ç»œç±»å‹
        """
        super(PF_MGCD, self).__init__()
        self.num_parts = num_parts
        self.feature_dim = feature_dim
        
        # 1. PCB Backbone - äººä½“éƒ¨ä»¶åˆ‡åˆ†
        # è¾“å…¥: [B, 3, 288, 144] -> è¾“å‡º: Kä¸ª [B, 2048, H/K, W]
        self.backbone = PCBBackbone(
            num_parts=num_parts, 
            pretrained=pretrained, 
            backbone=backbone
        )
        
        # 2. ISG-DM è§£è€¦æ¨¡å—
        # è¾“å…¥: [B, 2048, H, W] -> è¾“å‡º: ([B, D], [B, D_mod])
        # åˆ†ç¦»èº«ä»½ç‰¹å¾å’Œæ¨¡æ€ç‰¹å¾
        self.isg_dm = MultiPartISG_DM(
            num_parts=num_parts,
            input_dim=2048,       # ResNet50çš„è¾“å‡ºé€šé“æ•°
            id_dim=feature_dim,   # èº«ä»½ç‰¹å¾ç»´åº¦
            mod_dim=feature_dim   # æ¨¡æ€ç‰¹å¾ç»´åº¦
        )
        
        # 3. Transformer éƒ¨ä»¶ä¸Šä¸‹æ–‡äº¤äº’
        self.part_context = PartContextTransformer(
            feature_dim=feature_dim,
            nhead=8,
            num_layers=1
        )
        
        # 4. BNNeck - Batch Normalization Neck
        # ç”¨äºç‰¹å¾å½’ä¸€åŒ–ï¼Œæå‡åº¦é‡å­¦ä¹ æ€§èƒ½
        self.bottlenecks = nn.ModuleList([
            nn.BatchNorm1d(feature_dim) for _ in range(num_parts)
        ])
        self.bottlenecks.apply(weights_init_kaiming)
        
        # Dropout - è®­ç»ƒæ—¶éšæœºå¤±æ´»ï¼Œå¢å¼ºæ³›åŒ–
        self.dropout = nn.Dropout(p=0.5)
        
        # 5. åˆ†ç±»å™¨ - æ¯ä¸ªéƒ¨ä»¶ç‹¬ç«‹çš„IDåˆ†ç±»å¤´
        self.id_classifiers = nn.ModuleList([
            nn.Linear(feature_dim, num_identities, bias=False) 
            for _ in range(num_parts)
        ])
        self.id_classifiers.apply(weights_init_kaiming)
        
        # 6. å¤šç²’åº¦è®°å¿†åº“
        self.memory_bank = MultiPartMemoryBank(
            num_parts=num_parts,
            num_identities=num_identities,
            feature_dim=feature_dim,
            momentum=memory_momentum
        )
        
        # 7. è‡ªé€‚åº”å›¾ä¼ æ’­
        self.graph_propagation = AdaptiveGraphPropagation(
            temperature=temperature,
            top_k=top_k,
            use_entropy_weight=True,
            scale=30.0
        )
        
        # éƒ¨ä»¶æƒé‡ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼Œç”¨äºåŠ æƒèåˆï¼‰
        self.part_weights = nn.Parameter(torch.ones(num_parts))
    
    def forward(self, x, labels=None, **kwargs):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: Tensor[B, 3, H, W]ï¼Œè¾“å…¥å›¾åƒ
            labels: Tensor[B]ï¼ŒGround Truthæ ‡ç­¾ï¼ˆè®­ç»ƒæ—¶éœ€è¦ï¼‰
        
        Returns:
            outputs: Dictï¼ŒåŒ…å«:
                - 'id_features': List[Tensor[B, D]]ï¼ŒKä¸ªéƒ¨ä»¶çš„èº«ä»½ç‰¹å¾
                - 'mod_features': List[Tensor[B, D]]ï¼ŒKä¸ªéƒ¨ä»¶çš„æ¨¡æ€ç‰¹å¾
                - 'id_logits': List[Tensor[B, N]]ï¼Œå¸¦Dropoutçš„åˆ†ç±»logits
                - 'graph_logits': List[Tensor[B, N]]ï¼Œä¸å¸¦Dropoutçš„åˆ†ç±»logits
                - 'soft_labels': List[Tensor[B, N]]ï¼Œå›¾ä¼ æ’­ç”Ÿæˆçš„è½¯æ ‡ç­¾
                - 'entropy_weights': List[Tensor[B]]ï¼Œç†µæƒé‡
        """
        # 1. Backboneæå–éƒ¨ä»¶ç‰¹å¾
        part_features, _ = self.backbone(x)  # List of [B, 2048, H, W]
        
        # 2. ISG-DM èº«ä»½/æ¨¡æ€è§£è€¦
        id_features_raw, mod_features = self.isg_dm(part_features)
        # id_features_raw: List of [B, D] - çº¯èº«ä»½ç‰¹å¾
        # mod_features: List of [B, D] - æ¨¡æ€/é£æ ¼ç‰¹å¾
        
        # 3. Transformer éƒ¨ä»¶ä¸Šä¸‹æ–‡äº¤äº’
        id_features = self.part_context(id_features_raw)
        # id_features: List of [B, D] - äº¤äº’å¢å¼ºåçš„ç‰¹å¾
        
        # 4. åç»­æµç¨‹ï¼šBNNeck + Dropout + Classifier
        id_logits = []       # å¸¦Dropoutçš„logitsï¼ˆç”¨äºID Lossï¼‰
        graph_logits = []    # ä¸å¸¦Dropoutçš„logitsï¼ˆç”¨äºGraph Lossï¼‰
        
        for k in range(self.num_parts):
            # BNNeckå½’ä¸€åŒ–
            feat_bn = self.bottlenecks[k](id_features[k])  # [B, D]
            
            if self.training:
                # è®­ç»ƒæ¨¡å¼ï¼šåˆ†åˆ«è®¡ç®—ä¸¤ç§logits
                
                # Clean Logits -> Graph Lossï¼ˆè’¸é¦éœ€è¦ç¨³å®šé¢„æµ‹ï¼‰
                logit_clean = self.id_classifiers[k](feat_bn)
                graph_logits.append(logit_clean)
                
                # Dropout Logits -> ID Lossï¼ˆå¢å¼ºæ³›åŒ–ï¼‰
                feat_drop = self.dropout(feat_bn)
                logit_drop = self.id_classifiers[k](feat_drop)
                id_logits.append(logit_drop)
            else:
                # æµ‹è¯•æ¨¡å¼ï¼šä¸ä½¿ç”¨Dropout
                logit = self.id_classifiers[k](feat_bn)
                id_logits.append(logit)
                graph_logits.append(logit)
        
        # 5. å›¾ä¼ æ’­ç”Ÿæˆè½¯æ ‡ç­¾
        soft_labels, similarities, entropy_weights = self.graph_propagation(
            id_features, 
            self.memory_bank
        )
        
        # è¿”å›æ‰€æœ‰è¾“å‡º
        outputs = {
            'id_features': id_features,         # ç”¨äºTriplet Losså’Œè®°å¿†åº“æ›´æ–°
            'mod_features': mod_features,       # ä¿ç•™æ¥å£ï¼ˆå¯ç”¨äºæ¨¡æ€åˆ¤åˆ«ï¼‰
            'id_logits': id_logits,             # ç”¨äºID Loss
            'graph_logits': graph_logits,       # ç”¨äºGraph Distillation Loss
            'soft_labels': soft_labels,         # è½¯æ ‡ç­¾
            'entropy_weights': entropy_weights  # ç†µæƒé‡
        }
        
        return outputs
    
    def extract_features(self, x, pool_parts=True):
        """
        ç‰¹å¾æå–ï¼ˆæµ‹è¯•é˜¶æ®µï¼‰
        
        Args:
            x: Tensor[B, 3, H, W]ï¼Œè¾“å…¥å›¾åƒ
            pool_parts: boolï¼Œæ˜¯å¦æ‹¼æ¥æ‰€æœ‰éƒ¨ä»¶ç‰¹å¾
                       True: è¿”å› [B, K*D] æ‹¼æ¥ç‰¹å¾
                       False: è¿”å› [B, D] å¹³å‡ç‰¹å¾
        
        Returns:
            features: Tensor[B, K*D] or Tensor[B, D]
        """
        with torch.no_grad():
            # 1. å‰å‘ä¼ æ’­æå–ç‰¹å¾
            part_features, _ = self.backbone(x)
            id_features_raw, _ = self.isg_dm(part_features)
            id_features = self.part_context(id_features_raw)
            
            # 2. BNNeckå½’ä¸€åŒ–
            bn_features = []
            for k in range(self.num_parts):
                feat_bn = self.bottlenecks[k](id_features[k])  # [B, D]
                bn_features.append(feat_bn)
            
            # 3. L2å½’ä¸€åŒ–ï¼ˆä½™å¼¦è·ç¦»åº¦é‡ï¼‰
            norm_features = [F.normalize(f, p=2, dim=1) for f in bn_features]
            
            # 4. [ä¿®å¤] æ ¹æ®pool_partså‚æ•°é€‰æ‹©è¾“å‡ºæ–¹å¼
            if pool_parts:
                # æ‹¼æ¥æ‰€æœ‰éƒ¨ä»¶ç‰¹å¾ [B, K*D]
                return torch.cat(norm_features, dim=1)
            else:
                # å–æ‰€æœ‰éƒ¨ä»¶çš„å¹³å‡ [B, D]
                stacked = torch.stack(norm_features, dim=1)  # [B, K, D]
                return stacked.mean(dim=1)  # [B, D]
    
    def initialize_memory(self, dataloader, device, teacher_model=None):
        """
        åˆå§‹åŒ–è®°å¿†åº“
        
        é€šå¸¸åœ¨è®­ç»ƒå¼€å§‹å‰è°ƒç”¨ï¼Œä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†çš„RGBæ¨¡æ€æ•°æ®
        æ‰¹é‡è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„ç‰¹å¾å¹¶å­˜å…¥è®°å¿†åº“
        
        Args:
            dataloader: DataLoaderï¼Œè®­ç»ƒæ•°æ®åŠ è½½å™¨
            device: torch.deviceï¼Œè®¡ç®—è®¾å¤‡
            teacher_model: æ•™å¸ˆæ¨¡å‹ï¼ˆå¯é€‰ï¼Œæš‚æœªä½¿ç”¨ï¼‰
        """
        self.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–è®°å¿†åº“...")
        
        # æ”¶é›†æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
        all_features = [[] for _ in range(self.num_parts)]
        all_labels = []
        
        with torch.no_grad():
            for batch in dataloader:
                # æ•°æ®è§£åŒ…ï¼ˆå…¼å®¹å¤šç§æ ¼å¼ï¼‰
                if len(batch) == 3:
                    imgs, pids, _ = batch
                else:
                    imgs, info = batch
                    pids = info[:, 1]
                
                imgs = imgs.to(device)
                
                # å®Œæ•´å‰å‘ä¼ æ’­æå–ç‰¹å¾
                part_features, _ = self.backbone(imgs)
                id_features_raw, _ = self.isg_dm(part_features)
                id_features = self.part_context(id_features_raw)
                
                # æ”¶é›†æ¯ä¸ªéƒ¨ä»¶çš„ç‰¹å¾
                for k in range(self.num_parts):
                    all_features[k].append(id_features[k].cpu())
                all_labels.append(pids)
        
        # æ‹¼æ¥æ‰€æœ‰batch
        for k in range(self.num_parts):
            all_features[k] = torch.cat(all_features[k], dim=0).to(device)
        all_labels = torch.cat(all_labels, dim=0).long().to(device)
        
        # æ‰¹é‡åˆå§‹åŒ–è®°å¿†åº“
        self.memory_bank.initialize_memory(all_features, all_labels)
        
        self.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
        print("âœ… è®°å¿†åº“åˆå§‹åŒ–å®Œæˆ!")


# ===== æµ‹è¯•ä»£ç  =====
if __name__ == "__main__":
    print("="*60)
    print("PF-MGCD æ¨¡å‹æµ‹è¯•")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹
    model = PF_MGCD(
        num_parts=6,
        num_identities=395,
        feature_dim=256,
        pretrained=False  # æµ‹è¯•æ—¶ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡
    )
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters()) / 1e6
    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:.2f}M")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 4
    x = torch.randn(batch_size, 3, 288, 144)
    labels = torch.randint(0, 395, (batch_size,))
    
    print(f"\nè¾“å…¥shape: {x.shape}")
    
    # è®­ç»ƒæ¨¡å¼
    model.train()
    outputs = model(x, labels=labels)
    
    print(f"\nè¾“å‡ºå­—å…¸keys: {outputs.keys()}")
    print(f"id_featuresæ•°é‡: {len(outputs['id_features'])}")
    print(f"æ¯ä¸ªç‰¹å¾shape: {outputs['id_features'][0].shape}")
    print(f"id_logits shape: {outputs['id_logits'][0].shape}")
    print(f"soft_labels shape: {outputs['soft_labels'][0].shape}")
    
    # æµ‹è¯•æ¨¡å¼
    model.eval()
    feat_concat = model.extract_features(x, pool_parts=True)
    feat_avg = model.extract_features(x, pool_parts=False)
    
    print(f"\næ‹¼æ¥ç‰¹å¾shape: {feat_concat.shape}")
    print(f"å¹³å‡ç‰¹å¾shape: {feat_avg.shape}")
    
    print("\n" + "="*60)
    print("æ‰€æœ‰æµ‹è¯•é€šè¿‡! âœ…")
    print("="*60)
