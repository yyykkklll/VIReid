"""
æ¨¡å‹ç®¡ç†æ¨¡å—
============
åŠŸèƒ½ï¼š
1. ç»Ÿä¸€ç®¡ç† Backboneã€Classifierã€CLIP ç­‰æ¨¡å‹ç»„ä»¶
2. ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®
3. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
4. è®­ç»ƒ/è¯„ä¼°æ¨¡å¼åˆ‡æ¢

ä½œè€…:  ä¿®å¤ä¼˜åŒ–ç‰ˆæœ¬
æ—¥æœŸ: 2025-01-20
"""

import torch
import torch.nn as nn
import os

from .classifier import Image_Classifier
from .agw import AGW
from .clip_model import CLIP, load_clip_to_cpu
from .optim import WarmupMultiStepLR
from .loss import TripletLoss_WRT, Weak_loss
from utils import os_walk


# ==================== æ¨¡å‹æ³¨å†Œè¡¨ ====================

_models = {
    "resnet":  AGW,      # AGW (ResNet50 backbone)
    "clip": CLIP,       # CLIP-based model (ä¸æ¨èç”¨äºä¸»å¹²ç½‘ç»œ)
}


def create(args):
    """
    æ¨¡å‹å·¥å‚å‡½æ•°
    
    Args:
        args: é…ç½®å‚æ•°
        
    Returns:
        Model å®ä¾‹
    """
    if args.arch not in _models:
        raise KeyError(f"Unknown backbone: {args.arch}. Available: {list(_models.keys())}")
    
    print(f"ğŸ—ï¸  Creating model with backbone: {args.arch}")
    return Model(args)


# ==================== ä¸»æ¨¡å‹ç±» ====================

class Model: 
    """
    è·¨æ¨¡æ€è¡Œäººé‡è¯†åˆ«æ¨¡å‹ç®¡ç†å™¨
    
    ç»„ä»¶ï¼š
    - Backbone:  ç‰¹å¾æå–ç½‘ç»œ (AGW/ResNet50)
    - Classifier1: RGB åˆ†ç±»å™¨
    - Classifier2: IR åˆ†ç±»å™¨
    - Classifier3: è·¨æ¨¡æ€åˆ†ç±»å™¨
    - CLIP (å¯é€‰): è¯­ä¹‰å¢å¼ºæ¨¡å—
    """
    
    def __init__(self, args):
        self.args = args
        self. mode = args.mode
        self. device = torch.device(args.device if torch.cuda.is_available() else "cpu")
        self.save_path = os.path.join(args.save_path, "models/")
        
        # è®­ç»ƒè¶…å‚æ•°
        self.lr = args.lr
        self. weight_decay = args.weight_decay
        self.milestones = args.milestones
        self.resume = args.resume
        
        # ==================== æ„å»ºæ¨¡å‹ç»„ä»¶ ====================
        
        print(f"ğŸ“¦ Building backbone: {args.arch}")
        self.model = _models[args.arch](args).to(self.device)
        
        print(f"ğŸ“¦ Building classifiers (num_classes={args.num_classes})")
        self.classifier1 = Image_Classifier(args).to(self.device)  # RGB åˆ†ç±»å™¨
        self.classifier2 = Image_Classifier(args).to(self.device)  # IR åˆ†ç±»å™¨
        self. classifier3 = Image_Classifier(args).to(self.device)  # è·¨æ¨¡æ€åˆ†ç±»å™¨
        self.enable_cls3 = False  # Phase1 ä¸ä½¿ç”¨ classifier3
        
        # ==================== CLIP è¯­ä¹‰æ¨¡å—ï¼ˆå¯é€‰ï¼‰====================
        
        self.clip_model = None
        if hasattr(args, 'use_clip') and args.use_clip:
            print("ğŸ¨ Loading CLIP as Semantic Referee...")
            self.clip_model = self._build_clip_model(args)
            print(f"âœ… CLIP loaded successfully")
        
        # ==================== ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•° ====================
        
        self._init_optimizer()
        self._init_criterion()
        
        print(f"âœ… Model initialized on {self.device}")
    
    
    def _build_clip_model(self, args):
        """
        æ„å»º CLIP æ¨¡å‹ï¼ˆä¿®å¤ç‰ˆï¼‰
        
        å…³é”®ä¿®å¤ï¼š
        1. æ­£ç¡®è®¡ç®—ç‰¹å¾å›¾åˆ†è¾¨ç‡
        2. é€‚é…ä¸åŒçš„è¾“å…¥å›¾åƒå°ºå¯¸
        """
        # è®¡ç®— CLIP çš„ç‰¹å¾å›¾åˆ†è¾¨ç‡
        # ResNet50: stride=32, æ‰€ä»¥è¾“å‡ºåˆ†è¾¨ç‡ = (H-32)/32 + 1
        # ä¾‹å¦‚:  288x144 -> (288-32)/32+1 = 9, (144-32)/32+1 = 4
        h_resolution = (args.img_h - 32) // 32 + 1
        w_resolution = (args.img_w - 32) // 32 + 1
        
        print(f"   CLIP feature map resolution: {h_resolution}x{w_resolution}")
        print(f"   Input image resolution: {args.img_h}x{args.img_w}")
        
        clip_model = load_clip_to_cpu(
            backbone_name='RN50',
            h_resolution=h_resolution,
            w_resolution=w_resolution,
            vision_stride_size=32  # ResNet50 çš„æ€» stride
        )
        
        clip_model.to(self.device)
        clip_model.eval()
        
        # å†»ç»“æ‰€æœ‰å‚æ•°ï¼ˆCLIP ä»…ç”¨äºç‰¹å¾æå–ï¼‰
        for param in clip_model.parameters():
            param.requires_grad = False
        
        return clip_model
    
    
    def _init_optimizer(self):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆåŒé˜¶æ®µï¼‰
        
        Phase1: Backbone + Classifier1 + Classifier2
        Phase2: Phase1 + Classifier3
        """
        # Phase1 å‚æ•°ç»„
        params_phase1 = []
        for module in [self.model, self.classifier1, self.classifier2]:
            for name, param in module.named_parameters():
                if not param.requires_grad:
                    continue
                
                # åˆ†ç±»å™¨å±‚ä½¿ç”¨ 2x å­¦ä¹ ç‡
                if 'classifier' in name:
                    params_phase1.append({
                        'params': [param],
                        'lr': 2.0 * self.lr,
                        'weight_decay': self.weight_decay
                    })
                else:
                    params_phase1.append({
                        'params': [param],
                        'lr': self.lr,
                        'weight_decay': self.weight_decay
                    })
        
        # Phase2 å‚æ•°ç»„ï¼ˆåŒ…å« Phase1 + Classifier3ï¼‰
        params_phase2 = params_phase1.copy()
        for name, param in self.classifier3.named_parameters():
            if param.requires_grad:
                params_phase2.append({
                    'params': [param],
                    'lr': 2.0 * self.lr,
                    'weight_decay': self.weight_decay
                })
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self. optimizer_phase1 = torch.optim.Adam(params_phase1)
        self.optimizer_phase2 = torch.optim.Adam(params_phase2)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler_phase1 = WarmupMultiStepLR(
            self.optimizer_phase1,
            milestones=self. milestones,
            gamma=0.1,
            warmup_factor=0.01,
            warmup_iters=10,
            mode='cls'
        )
        self.scheduler_phase2 = WarmupMultiStepLR(
            self.optimizer_phase2,
            milestones=self.milestones,
            gamma=0.1,
            warmup_factor=0.01,
            warmup_iters=10,
            mode='cls'
        )
    
    
    def _init_criterion(self):
        """
        åˆå§‹åŒ–æŸå¤±å‡½æ•°
        """
        self.pid_criterion = nn.CrossEntropyLoss()
        self.tri_criterion = TripletLoss_WRT()
        self.weak_criterion = Weak_loss()
    
    
    # ==================== æ¨¡å¼åˆ‡æ¢ ====================
    
    def set_train(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.model.train()
        self.classifier1.train()
        self.classifier2.train()
        if self.enable_cls3:
            self.classifier3.train()
    
    
    def set_eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.model.eval()
        self.classifier1.eval()
        self.classifier2.eval()
        self.classifier3.eval()
    
    
    # ==================== æ¨¡å‹ä¿å­˜ä¸åŠ è½½ ====================
    
    def save_model(self, epoch, is_best=False):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args: 
            epoch: å½“å‰è½®æ¬¡
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        """
        if not os.path.exists(self.save_path):
            os.makedirs(self.save_path)
        
        # æ„å»ºçŠ¶æ€å­—å…¸
        state_dict = {
            'epoch':  epoch,
            'backbone':  self.model.state_dict(),
            'classifier1': self. classifier1.state_dict(),
            'classifier2': self.classifier2.state_dict(),
            'classifier3': self.classifier3.state_dict(),
            'optimizer_phase1': self.optimizer_phase1.state_dict(),
            'optimizer_phase2': self. optimizer_phase2.state_dict(),
        }
        
        if is_best:
            # ä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹
            model_path = os.path.join(self.save_path, 'model_best.pth')
            torch.save(state_dict, model_path)
            print(f"ğŸ’¾ Best model saved to {model_path}")
            
            # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            self._cleanup_old_models(keep_best=True)
        else:
            # å®šæœŸä¿å­˜
            model_path = os.path.join(self.save_path, f'model_{epoch}.pth')
            torch.save(state_dict, model_path)
            print(f"ğŸ’¾ Checkpoint saved to {model_path}")
    
    
    def resume_model(self, model_path=None):
        """
        åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            model_path: æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼ŒNone åˆ™è‡ªåŠ¨åŠ è½½æœ€æ–°æ¨¡å‹
        """
        if model_path is None:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
            model_path = self._find_latest_model()
        
        if model_path is None or not os.path.exists(model_path):
            print("âš ï¸  No checkpoint found, starting from scratch")
            return
        
        print(f"ğŸ“‚ Loading checkpoint from {model_path}")
        
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            self.model.load_state_dict(checkpoint['backbone'], strict=False)
            self.classifier1.load_state_dict(checkpoint['classifier1'], strict=False)
            self.classifier2.load_state_dict(checkpoint['classifier2'], strict=False)
            
            # å°è¯•åŠ è½½ classifier3ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
            if 'classifier3' in checkpoint:
                self.classifier3.load_state_dict(checkpoint['classifier3'], strict=False)
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            if self.mode == 'train' and 'optimizer_phase2' in checkpoint:
                try:
                    self.optimizer_phase2.load_state_dict(checkpoint['optimizer_phase2'])
                except:
                    print("âš ï¸  Failed to load optimizer state, using fresh optimizer")
            
            print(f"âœ… Model loaded successfully from epoch {checkpoint. get('epoch', 'unknown')}")
            
        except Exception as e:
            print(f"âŒ Failed to load checkpoint: {e}")
            print("   Starting from scratch...")
    
    
    def _find_latest_model(self):
        """
        æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Returns:
            æ¨¡å‹è·¯å¾„æˆ– None
        """
        if not os.path.exists(self.save_path):
            return None
        
        root, _, files = os_walk(self.save_path)
        
        # è¿‡æ»¤ .pth æ–‡ä»¶
        pth_files = [f for f in files if f.endswith('. pth')]
        
        if not pth_files:
            return None
        
        # ä¼˜å…ˆåŠ è½½ best æ¨¡å‹
        if 'model_best.pth' in pth_files:
            return os.path.join(root, 'model_best.pth')
        
        # å¦åˆ™åŠ è½½æœ€æ–°çš„ epoch æ¨¡å‹
        epochs = []
        for f in pth_files:
            try:
                epoch = int(f.replace('.pth', '').split('_')[-1])
                epochs.append((epoch, f))
            except: 
                continue
        
        if epochs:
            latest_file = max(epochs, key=lambda x: x[0])[1]
            return os. path.join(root, latest_file)
        
        return None
    
    
    def _cleanup_old_models(self, keep_best=True, keep_recent=3):
        """
        æ¸…ç†æ—§çš„æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args: 
            keep_best: æ˜¯å¦ä¿ç•™ best æ¨¡å‹
            keep_recent: ä¿ç•™æœ€è¿‘ N ä¸ªæ¨¡å‹
        """
        if not os.path.exists(self.save_path):
            return
        
        root, _, files = os_walk(self.save_path)
        pth_files = [f for f in files if f.endswith('. pth')]
        
        # æå– epoch ä¿¡æ¯
        epoch_files = []
        for f in pth_files:
            if f == 'model_best.pth' and keep_best:
                continue  # ä¿ç•™ best æ¨¡å‹
            
            try:
                epoch = int(f.replace('.pth', '').split('_')[-1])
                epoch_files.append((epoch, f))
            except:
                continue
        
        # æŒ‰ epoch æ’åº
        epoch_files.sort(key=lambda x: x[0], reverse=True)
        
        # åˆ é™¤æ—§æ¨¡å‹
        for epoch, filename in epoch_files[keep_recent:]:
            file_path = os.path.join(root, filename)
            try:
                os.remove(file_path)
                print(f"ğŸ—‘ï¸  Removed old checkpoint: {filename}")
            except:
                pass
    
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    def count_parameters(self):
        """
        ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡
        
        Returns:
            total:  æ€»å‚æ•°é‡
            trainable: å¯è®­ç»ƒå‚æ•°é‡
        """
        total = sum(p.numel() for p in self.model.parameters())
        trainable = sum(p. numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š Model parameters:")
        print(f"   Total: {total / 1e6:.2f}M")
        print(f"   Trainable: {trainable / 1e6:.2f}M")
        
        return total, trainable
    
    
    def get_learning_rate(self):
        """
        è·å–å½“å‰å­¦ä¹ ç‡
        """
        return self.optimizer_phase2.param_groups[0]['lr']