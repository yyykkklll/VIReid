"""
è·¨æ¨¡æ€åŒ¹é…èšåˆæ¨¡å— (Cross-Modal Match Aggregation)
=================================================
åŠŸèƒ½ï¼š
1. æå– RGB å’Œ IR æ¨¡æ€çš„ç‰¹å¾
2. ä½¿ç”¨ Sinkhorn ç®—æ³•è¿›è¡Œå…¨å±€æœ€ä¼˜åŒ¹é…
3. æ”¯æŒ CLIP è¯­ä¹‰ç‰¹å¾å¢å¼ºåŒ¹é…
4. ç®¡ç†ç‰¹å¾è®°å¿†åº“ç”¨äºä¸€è‡´æ€§çº¦æŸ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import Counter, OrderedDict


class CMA(nn.Module):
    """
    è·¨æ¨¡æ€åŒ¹é…èšåˆå™¨
    """
    def __init__(self, args):
        super(CMA, self).__init__()
        self.device = torch.device(args.device)
        self.num_classes = args.num_classes
        self.T = args.temperature           # Softmax æ¸©åº¦å‚æ•°
        self.sigma = args.sigma             # è®°å¿†åº“åŠ¨é‡æ›´æ–°ç³»æ•°
        self.args = args
        
        # ç‰¹å¾è®°å¿†åº“ï¼šå­˜å‚¨æ¯ä¸ªç±»åˆ«çš„å¹³å‡ç‰¹å¾ (ç”¨äºä¸€è‡´æ€§çº¦æŸ)
        self.register_buffer('vis_memory', torch.zeros(self.num_classes, 2048))
        self.register_buffer('ir_memory', torch. zeros(self.num_classes, 2048))
        
        # CLIP ç‰¹å¾ç¼“å­˜ (ç”¨äºè¯­ä¹‰åŒ¹é…)
        self.vis_clip_feats = None
        self.ir_clip_feats = None
        
        # å†…éƒ¨çŠ¶æ€
        self.not_saved = True
        self.mode = None
        

    # ==================== æ ¸å¿ƒæ¥å£ ====================
    
    def extract_and_match(self, model, dataset, clip_model=None):
        """
        ç»Ÿä¸€æ¥å£ï¼šæå–ç‰¹å¾å¹¶æ‰§è¡ŒåŒ¹é…
        
        Args:
            model: è®­ç»ƒæ¨¡å‹
            dataset: æ•°æ®é›†å¯¹è±¡
            clip_model:  CLIP æ¨¡å‹ (å¯é€‰)
            
        Returns:
            v2i_dict: RGB -> IR åŒ¹é…å­—å…¸
            i2v_dict: IR -> RGB åŒ¹é…å­—å…¸
        """
        # 1. æå–ç‰¹å¾
        self._extract_features(model, dataset, clip_model)
        
        # 2. æ‰§è¡ŒåŒ¹é…
        if hasattr(self. args, 'use_sinkhorn') and self.args.use_sinkhorn:
            print("ğŸ”— ä½¿ç”¨ Sinkhorn ç®—æ³•è¿›è¡Œå…¨å±€æœ€ä¼˜åŒ¹é…...")
            return self._match_sinkhorn()
        else:
            print("ğŸ”— ä½¿ç”¨è´ªå©ªç®—æ³•è¿›è¡Œå¿«é€ŸåŒ¹é…...")
            return self._match_greedy()
    

    # ==================== ç‰¹å¾æå– ====================
    
    @torch.no_grad()
    def _extract_features(self, model, dataset, clip_model=None):
        """
        æå– RGB å’Œ IR æ¨¡æ€çš„ç‰¹å¾
        """
        model.set_eval()
        rgb_loader, ir_loader = dataset.get_normal_loader()
        
        print("ğŸ“Š æå– RGB ç‰¹å¾...")
        rgb_feats, rgb_labels, rgb_cls, rgb_clip = self._extract_single_modal(
            model, rgb_loader, 'rgb', clip_model)
        
        print("ğŸ“Š æå– IR ç‰¹å¾...")
        ir_feats, ir_labels, ir_cls, ir_clip = self._extract_single_modal(
            model, ir_loader, 'ir', clip_model)
        
        # ä¿å­˜åˆ°å†…éƒ¨çŠ¶æ€
        self._save_features(
            rgb_cls, ir_cls, rgb_labels, ir_labels, 
            rgb_feats, ir_feats, rgb_clip, ir_clip
        )
    
    
    @torch.no_grad()
    def _extract_single_modal(self, model, loader, modal, clip_model=None):
        """
        æå–å•ä¸ªæ¨¡æ€çš„ç‰¹å¾
        
        Returns:
            features: BN ç‰¹å¾ [N, 2048]
            labels: ä¼ªæ ‡ç­¾ [N]
            cls_scores: åˆ†ç±»åˆ†æ•° [N, num_classes]
            clip_feats: CLIP ç‰¹å¾ [N, 1024] (å¦‚æœå¯ç”¨)
        """
        all_features = []
        all_labels = []
        all_cls_scores = []
        all_clip_feats = []
        
        for imgs_list, infos in loader:
            labels = infos[: , 1]. to(self.device)
            
            # å¤„ç†æ•°æ®å¢å¼ºçš„æƒ…å†µ
            if isinstance(imgs_list, (list, tuple)):
                imgs = imgs_list[0]. to(self.device)  # åªç”¨åŸå§‹å›¾åƒ
            else: 
                imgs = imgs_list.to(self.device)
            
            # æå–ä»»åŠ¡æ¨¡å‹ç‰¹å¾
            _, bn_features = model. model(imgs)
            
            # æ ¹æ®æ¨¡æ€é€‰æ‹©åˆ†ç±»å™¨ (äº¤å‰åˆ†ç±»ç­–ç•¥)
            if modal == 'rgb':
                cls_scores, _ = model.classifier2(bn_features)  # RGB -> IR åˆ†ç±»å™¨
            else: 
                cls_scores, _ = model.classifier1(bn_features)  # IR -> RGB åˆ†ç±»å™¨
            
            all_features.append(bn_features. cpu())
            all_labels. append(labels. cpu())
            all_cls_scores.append(cls_scores. cpu())
            
            # æå– CLIP ç‰¹å¾ (å¦‚æœå¯ç”¨)
            if clip_model is not None: 
                clip_feats = self._extract_clip_features(clip_model, imgs)
                all_clip_feats.append(clip_feats)
        
        # åˆå¹¶æ‰€æœ‰ batch
        features = torch.cat(all_features, dim=0)
        labels = torch.cat(all_labels, dim=0)
        cls_scores = torch.cat(all_cls_scores, dim=0)
        clip_feats = torch.cat(all_clip_feats, dim=0) if all_clip_feats else None
        
        return features, labels, cls_scores, clip_feats
    
    
    @torch.no_grad()
    def _extract_clip_features(self, clip_model, imgs):
        """
        æå– CLIP è¯­ä¹‰ç‰¹å¾ (ä¿®å¤ç‰ˆæœ¬)
        
        å…³é”®ä¿®å¤ï¼š
        1. ç›´æ¥è°ƒç”¨ attnpoolï¼Œä¸åŠ ç´¢å¼•
        2. ç¡®ä¿è¾“å…¥æ˜¯æ ‡å‡†å½’ä¸€åŒ–çš„å›¾åƒ
        """
        # CLIP ç¼–ç å›¾åƒ
        feat_map = clip_model.encode_image(imgs)  # [Batch, 2048, H, W]
        
        # é€šè¿‡ Attention Pooling å¾—åˆ°å…¨å±€ç‰¹å¾
        if hasattr(clip_model. visual, 'attnpool'):
            # ResNet50 åˆ†æ”¯ï¼šattnpool ç›´æ¥è¿”å› [Batch, 1024]
            clip_emb = clip_model.visual.attnpool(feat_map)  # âœ… ä¿®å¤ï¼šå»æ‰ [0]
        else:
            # ViT åˆ†æ”¯ï¼šä½¿ç”¨ CLS token
            if isinstance(feat_map, tuple):
                clip_emb = feat_map[-1]  # å–æœ€åä¸€ä¸ªè¾“å‡º (é€šå¸¸æ˜¯æŠ•å½±åçš„ç‰¹å¾)
            else:
                clip_emb = feat_map. mean(dim=[-2, -1])  # å…¨å±€å¹³å‡æ± åŒ–
        
        return clip_emb. detach().cpu()
    
    
    @torch.no_grad()
    def _save_features(self, rgb_cls, ir_cls, rgb_labels, ir_labels, 
                       rgb_features, ir_features, clip_rgb, clip_ir):
        """
        ä¿å­˜ç‰¹å¾åˆ°å†…éƒ¨çŠ¶æ€å¹¶æ›´æ–°è®°å¿†åº“
        """
        self.mode = 'scores'
        self.not_saved = False
        
        # ä¿å­˜åˆ†ç±»åˆ†æ•° (ç”¨äºåŒ¹é…)
        self.vis = F.softmax(self.T * rgb_cls, dim=1).cpu().numpy()
        self.ir = F.softmax(self.T * ir_cls, dim=1).cpu().numpy()
        self.rgb_ids = rgb_labels.cpu()
        self.ir_ids = ir_labels.cpu()
        
        # æ›´æ–°ç‰¹å¾è®°å¿†åº“
        self._update_memory(rgb_features. to(self.device), ir_features.to(self.device),
                            rgb_labels.to(self. device), ir_labels.to(self.device))
        
        # ä¿å­˜ CLIP ç‰¹å¾
        if clip_rgb is not None and clip_ir is not None: 
            self.vis_clip_feats = clip_rgb
            self.ir_clip_feats = clip_ir
            print(f"âœ… CLIP ç‰¹å¾å·²ä¿å­˜: RGB {clip_rgb.shape}, IR {clip_ir.shape}")
        else:
            self.vis_clip_feats = None
            self.ir_clip_feats = None
    
    
    @torch.no_grad()
    def _update_memory(self, rgb_feats, ir_feats, rgb_labels, ir_labels):
        """
        ä½¿ç”¨ EMA æ›´æ–°ç‰¹å¾è®°å¿†åº“
        """
        self.vis_memory = self.vis_memory. to(self.device)
        self.ir_memory = self.ir_memory.to(self.device)
        
        # æ›´æ–° RGB è®°å¿†
        for label in torch.unique(rgb_labels):
            mask = (rgb_labels == label)
            if mask.any():
                new_feat = rgb_feats[mask]. mean(dim=0)
                self.vis_memory[label] = (1 - self.sigma) * self.vis_memory[label] + \
                                         self.sigma * new_feat
        
        # æ›´æ–° IR è®°å¿†
        for label in torch.unique(ir_labels):
            mask = (ir_labels == label)
            if mask.any():
                new_feat = ir_feats[mask]. mean(dim=0)
                self.ir_memory[label] = (1 - self.sigma) * self.ir_memory[label] + \
                                        self.sigma * new_feat
    

    # ==================== Sinkhorn åŒ¹é… ====================
    
    def _match_sinkhorn(self):
        """
        ä½¿ç”¨ Sinkhorn ç®—æ³•è¿›è¡Œå…¨å±€æœ€ä¼˜åŒ¹é…
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        1. æ„å»ºç›¸ä¼¼åº¦çŸ©é˜µ (ä¸“å®¶åˆ†æ•° + CLIP è¯­ä¹‰)
        2. Sinkhorn è¿­ä»£æ±‚è§£æœ€ä¼˜ä¼ è¾“
        3. åŸºäºä¼ è¾“çŸ©é˜µç”ŸæˆåŒ¹é…å­—å…¸
        """
        # 1. è®¡ç®—ä¸“å®¶ç›¸ä¼¼åº¦
        score_rgb = torch.from_numpy(self.vis).to(self.device)
        score_ir = torch.from_numpy(self.ir).to(self.device)
        score_rgb = F.normalize(score_rgb, dim=1)
        score_ir = F.normalize(score_ir, dim=1)
        sim_expert = torch.matmul(score_rgb, score_ir.T)  # [N_rgb, N_ir]
        
        # 2. èåˆ CLIP è¯­ä¹‰ç›¸ä¼¼åº¦ (å¦‚æœå¯ç”¨)
        if self.vis_clip_feats is not None and self.ir_clip_feats is not None:
            clip_rgb = F.normalize(self.vis_clip_feats. to(self.device), dim=1)
            clip_ir = F.normalize(self.ir_clip_feats.to(self.device), dim=1)
            sim_clip = torch.matmul(clip_rgb, clip_ir.T)
            
            w_clip = getattr(self.args, 'w_clip', 0.3)
            sim_final = (1 - w_clip) * sim_expert + w_clip * sim_clip
            print(f"ğŸ¯ CLIP æƒé‡:  {w_clip:.2f}")
        else:
            sim_final = sim_expert
        
        # 3. Log-domain Sinkhorn (æ•°å€¼ç¨³å®šç‰ˆæœ¬)
        epsilon = getattr(self.args, 'sinkhorn_reg', 0.05)
        log_Q = sim_final / epsilon
        
        # è¿­ä»£æ±‚è§£
        max_iters = 100
        tolerance = 1e-4
        for iteration in range(max_iters):
            log_Q_prev = log_Q.clone()
            
            # è¡Œå½’ä¸€åŒ– (log-domain)
            log_Q = log_Q - torch.logsumexp(log_Q, dim=1, keepdim=True)
            # åˆ—å½’ä¸€åŒ– (log-domain)
            log_Q = log_Q - torch.logsumexp(log_Q, dim=0, keepdim=True)
            
            # æ£€æŸ¥æ”¶æ•›
            if torch.abs(log_Q - log_Q_prev).max() < tolerance:
                print(f"âœ… Sinkhorn åœ¨ç¬¬ {iteration} è½®æ”¶æ•›")
                break
        
        Q = torch.exp(log_Q).cpu().numpy()
        
        # 4. ç”ŸæˆåŒ¹é…å­—å…¸ (ç½®ä¿¡åº¦é˜ˆå€¼ç­–ç•¥)
        confidence_threshold = 0.5
        v2i, i2v = self._generate_matches_from_Q(Q, confidence_threshold)
        
        print(f"ğŸ“Š åŒ¹é…ç»“æœ: RGB->IR {len(v2i)}/{len(self.rgb_ids)}, "
              f"IR->RGB {len(i2v)}/{len(self.ir_ids)}")
        
        return v2i, i2v
    
    
    def _generate_matches_from_Q(self, Q, threshold):
        """
        ä»ä¼ è¾“çŸ©é˜µç”ŸæˆåŒ¹é…å­—å…¸
        
        ç­–ç•¥ï¼šåŸºäºç½®ä¿¡åº¦é˜ˆå€¼çš„è½¯åŒ¹é… (ç›¸æ¯”ä¸¥æ ¼åŒå‘éªŒè¯æ›´å®½æ¾)
        """
        v2i = OrderedDict()
        i2v = OrderedDict()
        
        # RGB -> IR åŒ¹é…
        max_j = np.argmax(Q, axis=1)
        for i, j in enumerate(max_j):
            if Q[i, j] > threshold:   # ç½®ä¿¡åº¦è¶³å¤Ÿé«˜
                rgb_id = self.rgb_ids[i]. item()
                ir_id = self.ir_ids[j]. item()
                if rgb_id not in v2i:  # é¿å…é‡å¤
                    v2i[rgb_id] = ir_id
        
        # IR -> RGB åŒ¹é…
        max_i = np.argmax(Q, axis=0)
        for j, i in enumerate(max_i):
            if Q[i, j] > threshold: 
                rgb_id = self.rgb_ids[i].item()
                ir_id = self.ir_ids[j].item()
                if ir_id not in i2v: 
                    i2v[ir_id] = rgb_id
        
        return v2i, i2v
    

    # ==================== è´ªå©ªåŒ¹é… (å¤‡ç”¨) ====================
    
    def _match_greedy(self):
        """
        è´ªå©ªåŒ¹é…ç®—æ³• (ç”¨äºå¿«é€Ÿå®éªŒæˆ–æ¶ˆèç ”ç©¶)
        """
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        dists = np.matmul(self.vis, self.ir.T)
        
        # æ’åºå¹¶è´ªå©ªé€‰æ‹©
        sorted_indices = np.argsort(-dists, axis=None)
        sorted_2d = np.unravel_index(sorted_indices, dists.shape)
        idx_rgb, idx_ir = sorted_2d[0], sorted_2d[1]
        
        # ç»Ÿè®¡åŒ¹é…é¢‘ç‡
        pairs = [(self.rgb_ids[i]. item(), self.ir_ids[j].item()) 
                 for i, j in zip(idx_rgb, idx_ir)]
        pair_counts = Counter(pairs)
        
        # ç”Ÿæˆå”¯ä¸€åŒ¹é…
        v2i, i2v = OrderedDict(), OrderedDict()
        matched_rgb, matched_ir = set(), set()
        
        for (rgb_id, ir_id), count in pair_counts.most_common():
            if rgb_id not in matched_rgb and ir_id not in matched_ir:
                v2i[rgb_id] = ir_id
                i2v[ir_id] = rgb_id
                matched_rgb.add(rgb_id)
                matched_ir.add(ir_id)
        
        print(f"ğŸ“Š è´ªå©ªåŒ¹é…ç»“æœ: {len(v2i)} å¯¹")
        return v2i, i2v
    

    # ==================== å·¥å…·å‡½æ•° ====================
    
    def get_memory_features(self):
        """
        è·å–è®°å¿†åº“ç‰¹å¾ (ç”¨äºä¸€è‡´æ€§çº¦æŸ)
        """
        return self.vis_memory, self.ir_memory
    
    
    def reset(self):
        """
        é‡ç½®å†…éƒ¨çŠ¶æ€
        """
        self.not_saved = True
        self.vis_clip_feats = None
        self.ir_clip_feats = None